{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a42295-9c8a-4fca-ac80-7c343f577fb0",
   "metadata": {},
   "source": [
    "Simple policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "579bee88-babc-4733-bad5-80ebb1b99fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n",
    "        self.layers = [\n",
    "            nn.Linear(idim, odim)\n",
    "            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = mx.maximum(l(x), 0.0)\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "dcf5543a-5e50-4496-a42f-93c45610eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, X, y):\n",
    "    return mx.mean(nn.losses.cross_entropy(model(X), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "66d2cd8d-7ba9-4d4b-969b-0c1d20204e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, X, y):\n",
    "    return mx.mean(mx.argmax(model(X), axis=1) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "eb8593aa-4dbd-4376-b4ab-cf8166657a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterate(batch_size, X, y):\n",
    "    perm = mx.array(np.random.permutation(y.size))\n",
    "    for s in range(0, y.size, batch_size):\n",
    "        ids = perm[s : s + batch_size]\n",
    "        yield X[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "1c758db0-8393-4876-86e4-f294bd1201c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "hidden_dim = 32\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load the data\n",
    "import mnist\n",
    "train_images, train_labels, test_images, test_labels = map(\n",
    "    mx.array, mnist.mnist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f543bb3b-6f26-48c5-aacf-7e65e6183585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test accuracy 0.929\n",
      "Epoch 1: Test accuracy 0.940\n",
      "Epoch 2: Test accuracy 0.943\n",
      "Epoch 3: Test accuracy 0.949\n",
      "Epoch 4: Test accuracy 0.953\n",
      "Epoch 5: Test accuracy 0.956\n",
      "Epoch 6: Test accuracy 0.957\n",
      "Epoch 7: Test accuracy 0.959\n",
      "Epoch 8: Test accuracy 0.959\n",
      "Epoch 9: Test accuracy 0.959\n",
      "Epoch 10: Test accuracy 0.962\n",
      "Epoch 11: Test accuracy 0.963\n",
      "Epoch 12: Test accuracy 0.962\n",
      "Epoch 13: Test accuracy 0.963\n",
      "Epoch 14: Test accuracy 0.962\n",
      "Epoch 15: Test accuracy 0.963\n",
      "Epoch 16: Test accuracy 0.965\n",
      "Epoch 17: Test accuracy 0.964\n",
      "Epoch 18: Test accuracy 0.965\n",
      "Epoch 19: Test accuracy 0.966\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "# Get a function which gives the loss and gradient of the\n",
    "# loss with respect to the model's trainable parameters\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for X, y in batch_iterate(batch_size, train_images, train_labels):\n",
    "        loss, grads = loss_and_grad_fn(model, X, y)\n",
    "\n",
    "        # Update the optimizer state and model parameters\n",
    "        # in a single call\n",
    "        optimizer.update(model, grads)\n",
    "\n",
    "        # Force a graph evaluation\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "\n",
    "    accuracy = eval_fn(model, test_images, test_labels)\n",
    "    print(f\"Epoch {e}: Test accuracy {accuracy.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fe12b-8626-4a51-9267-dc195b5cf897",
   "metadata": {},
   "source": [
    "MLX is under control: it works!\n",
    "\n",
    "Now, it's time to figure out gymnasium and the environment.\n",
    "\n",
    "# MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "3825839a-2f0e-4e60-9f0e-d8354a7cabc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 4, 8], dtype=int32),\n",
       " array([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]], dtype=int32))"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mx.arange(9).reshape(3, 3)\n",
    "\n",
    "\n",
    "X[mx.arange(3), mx.arange(3)], X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "cee22a97-edec-44de-bb8a-a1c373e467c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=1e-2)\n",
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    probs = mx.softmax(logits, axis=0)\n",
    "    return probs, model\n",
    "\n",
    "def get_action(obs):\n",
    "    probs, _ = get_policy(obs)\n",
    "    return np.random.choice(list(range(len(probs))), 1, p=np.array(probs))[0]\n",
    "\n",
    "def compute_loss(obs, act, weights):\n",
    "    # calc log probs\n",
    "    probs, _ = get_policy(obs)\n",
    "    logp = mx.log(probs[mx.arange(probs.shape[0]), act])\n",
    "    return -(logp * weights).mean()\n",
    "\n",
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "loss_and_grads_fn = nn.value_and_grad(model, compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "d5643663-cefa-4d32-b946-d4bce2af0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    epoch_len = 1000\n",
    "    \n",
    "    batch_obs = []\n",
    "    batch_act = []\n",
    "    \n",
    "    batch_ret = []\n",
    "    batch_weights = []\n",
    "    batch_lens = []\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    ep_rews = []\n",
    "    \n",
    "    while True:\n",
    "        batch_obs.append(obs.copy())\n",
    "        act = get_action(mx.array(obs))\n",
    "    \n",
    "        obs, rew, term, _, _ = env.step(act)\n",
    "        \n",
    "        # Append to history\n",
    "        batch_act.append(act)\n",
    "        ep_rews.append(rew)\n",
    "    \n",
    "        if term:\n",
    "            # Sum for trajectories\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_ret.append(ep_ret)\n",
    "            batch_lens.append(len(ep_rews))\n",
    "            # Create weights\n",
    "            batch_weights += list(reward_to_go(ep_rews))\n",
    "            \n",
    "            if len(batch_obs) > epoch_len:\n",
    "                break\n",
    "    \n",
    "            (obs, _), term, ep_rews = env.reset(), False, []\n",
    "\n",
    "    loss, grads = loss_and_grads_fn(mx.array(np.array(batch_obs)),\n",
    "                                    mx.array(np.array(batch_act)), \n",
    "                                    mx.array(np.array(batch_weights)))\n",
    "    optimizer.update(model, grads)\n",
    "    mx.eval(model.parameters(), optimizer.state)\n",
    "                \n",
    "    return batch_ret, loss, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "05955995-4c9c-4a80-af92-83bce3bc268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 83.45491790771484; mean batch len = 19.705882352941178\n",
      "epoch 1: loss = 110.93253326416016; mean batch len = 24.853658536585368\n",
      "epoch 2: loss = 151.95423889160156; mean batch len = 34.266666666666666\n",
      "epoch 3: loss = 169.48532104492188; mean batch len = 44.47826086956522\n",
      "epoch 4: loss = 243.25856018066406; mean batch len = 62.11764705882353\n",
      "epoch 5: loss = 408.7662048339844; mean batch len = 103.0\n",
      "epoch 6: loss = 268.99761962890625; mean batch len = 71.85714285714286\n",
      "epoch 7: loss = 296.6059875488281; mean batch len = 80.84615384615384\n",
      "epoch 8: loss = 279.02294921875; mean batch len = 68.86666666666666\n",
      "epoch 9: loss = 220.41543579101562; mean batch len = 59.76470588235294\n",
      "epoch 10: loss = 248.4477996826172; mean batch len = 68.93333333333334\n",
      "epoch 11: loss = 254.44683837890625; mean batch len = 65.0\n",
      "epoch 12: loss = 223.99301147460938; mean batch len = 60.705882352941174\n",
      "epoch 13: loss = 260.1882629394531; mean batch len = 66.25\n",
      "epoch 14: loss = 203.06796264648438; mean batch len = 54.63157894736842\n",
      "epoch 15: loss = 174.22161865234375; mean batch len = 48.04761904761905\n",
      "epoch 16: loss = 105.25493621826172; mean batch len = 28.416666666666668\n",
      "epoch 17: loss = 65.25537872314453; mean batch len = 17.220338983050848\n",
      "epoch 18: loss = 47.32676315307617; mean batch len = 12.451219512195122\n",
      "epoch 19: loss = 39.00918197631836; mean batch len = 10.34020618556701\n",
      "epoch 20: loss = 36.863929748535156; mean batch len = 9.634615384615385\n",
      "epoch 21: loss = 37.551124572753906; mean batch len = 9.852941176470589\n",
      "epoch 22: loss = 38.55766296386719; mean batch len = 10.191919191919192\n",
      "epoch 23: loss = 39.63359832763672; mean batch len = 10.489583333333334\n",
      "epoch 24: loss = 42.30160140991211; mean batch len = 11.188888888888888\n",
      "epoch 25: loss = 44.9219856262207; mean batch len = 11.928571428571429\n",
      "epoch 26: loss = 46.377403259277344; mean batch len = 12.37037037037037\n",
      "epoch 27: loss = 51.07858657836914; mean batch len = 13.407894736842104\n",
      "epoch 28: loss = 53.84816360473633; mean batch len = 14.169014084507042\n",
      "epoch 29: loss = 60.42945861816406; mean batch len = 15.859375\n",
      "epoch 30: loss = 56.477745056152344; mean batch len = 15.0\n",
      "epoch 31: loss = 57.80685043334961; mean batch len = 15.149253731343284\n",
      "epoch 32: loss = 60.01450729370117; mean batch len = 15.646153846153846\n",
      "epoch 33: loss = 57.50961685180664; mean batch len = 15.074626865671641\n",
      "epoch 34: loss = 69.8230972290039; mean batch len = 16.833333333333332\n",
      "epoch 35: loss = 63.071800231933594; mean batch len = 15.952380952380953\n",
      "epoch 36: loss = 60.20732116699219; mean batch len = 15.646153846153846\n",
      "epoch 37: loss = 62.93323516845703; mean batch len = 16.161290322580644\n",
      "epoch 38: loss = 64.44486236572266; mean batch len = 16.225806451612904\n",
      "epoch 39: loss = 59.336448669433594; mean batch len = 14.385714285714286\n",
      "epoch 40: loss = 61.378814697265625; mean batch len = 15.538461538461538\n",
      "epoch 41: loss = 94.47062683105469; mean batch len = 17.050847457627118\n",
      "epoch 42: loss = 55.56950378417969; mean batch len = 13.930555555555555\n",
      "epoch 43: loss = 53.37840270996094; mean batch len = 13.171052631578947\n",
      "epoch 44: loss = 54.99353790283203; mean batch len = 13.346666666666666\n",
      "epoch 45: loss = 49.20928192138672; mean batch len = 12.378048780487806\n",
      "epoch 46: loss = 45.73506164550781; mean batch len = 11.952380952380953\n",
      "epoch 47: loss = 46.33128356933594; mean batch len = 11.69767441860465\n",
      "epoch 48: loss = 44.978336334228516; mean batch len = 11.563218390804598\n",
      "epoch 49: loss = 49.523719787597656; mean batch len = 12.506172839506172\n"
     ]
    }
   ],
   "source": [
    "for e in range(50):\n",
    "    ret, loss, batch_lens = train_one_epoch()\n",
    "    print(f\"epoch {e}: loss = {loss.item()}; mean batch len = {np.mean(batch_lens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf8572-a244-4b63-9c71-99d82437509c",
   "metadata": {},
   "source": [
    "Please do not use it yet...\n",
    "\n",
    "I think a lot of utility functions existing in PyTorch are not yet implemented in MLX, which makes it quite hard for the development.\n",
    "\n",
    "The lazy evaluation and compilation were also quite confusing, which made the whole experience quite unpleasent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537d1b0-f5f2-4ff5-a886-b4784845fb0b",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "5a9912cf-0fcb-4f5c-8cae-c6f1f4e40b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: loss = 10.730097514449382; mean batch len = 21.857142857142858\n",
      "epoch 19: loss = 22.730474894486584; mean batch len = 51.5\n",
      "epoch 29: loss = 17.588762214376587; mean batch len = 37.875\n",
      "epoch 39: loss = 15.48880399016439; mean batch len = 39.666666666666664\n",
      "epoch 49: loss = 18.680067237668904; mean batch len = 52.166666666666664\n",
      "epoch 59: loss = 22.067168519978928; mean batch len = 62.2\n",
      "epoch 69: loss = 32.871211015335895; mean batch len = 86.25\n",
      "epoch 79: loss = 17.16089697937649; mean batch len = 46.0\n",
      "epoch 89: loss = 36.00716688901583; mean batch len = 103.25\n",
      "epoch 99: loss = 84.48073412971387; mean batch len = 302.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    return logits\n",
    "\n",
    "# make action selection function (outputs int actions, sampled from policy)\n",
    "def get_action(obs):\n",
    "    logits = get_policy(obs)\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    probs = np.array(probs.detach()) \n",
    "    \n",
    "    return np.random.choice(range(2), p=probs)\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    probs = F.softmax(get_policy(obs), dim=-1)\n",
    "    logp = torch.log(probs[range(probs.shape[0]), act])\n",
    "        \n",
    "    return -(logp * weights).mean()\n",
    "\n",
    "\n",
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    epoch_len = 300\n",
    "    \n",
    "    batch_obs = []\n",
    "    batch_act = []\n",
    "    \n",
    "    batch_ret = []\n",
    "    batch_weights = []\n",
    "    batch_lens = []\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    ep_rews = []\n",
    "    \n",
    "    while True:\n",
    "        batch_obs.append(obs.copy())\n",
    "        act = get_action(torch.tensor(obs))\n",
    "    \n",
    "        obs, rew, term, _, _ = env.step(act)\n",
    "        \n",
    "        # Append to history\n",
    "        batch_act.append(act)\n",
    "        ep_rews.append(rew)\n",
    "    \n",
    "        if term:\n",
    "            # Sum for trajectories\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_ret.append(ep_ret)\n",
    "            batch_lens.append(len(ep_rews))\n",
    "            # Create weights\n",
    "            batch_weights += list(reward_to_go(ep_rews))\n",
    "            \n",
    "            if len(batch_obs) > epoch_len:\n",
    "                break\n",
    "    \n",
    "            (obs, _), term, ep_rews = env.reset(), False, []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss(torch.tensor(np.array(batch_obs)),\n",
    "                        torch.tensor(np.array(batch_act)), \n",
    "                        torch.tensor(np.array(batch_weights)))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "                \n",
    "    return batch_ret, loss, batch_lens\n",
    "\n",
    "render = False\n",
    "epochs = 100\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    # batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "    ret, loss, batch_lens = train_one_epoch()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"epoch {i}: loss = {loss.item()}; mean batch len = {np.mean(batch_lens)}\")\n",
    "    # print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "            # (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
